{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Spark\n",
    "\n",
    "Have you ever wondered how Apache Spark and Beam work under the cover? Both Spark and Beam are high-performance, data-parallel processing frameworks generally run on a compute cluster. Basically, they are used for ETL to maximize throughput.  \n",
    "<br>\n",
    "Parallelism can occur at the **chip** level: Single instruction, multiple data (SIMD) instructions can utilize the full width of the registers on a CPU (NumPy) and GPU (TensorFlow).  \n",
    "Parallelism can also occur at the **process** level using libraries such as `multiprocessing`, `ipyparallel`, and `MRJob`. You can take a look at my comparision between ipyparallel and MRJob: https://github.com/eugeneh101/ipyparallel-vs-MRJob  \n",
    "Parallelism can also occur at the **machine** level where multiple machines work cooperatively in a cluster--that's what Spark and Beam do.  \n",
    "<br>\n",
    "I'm deeply curious about the implementation underlying Spark in which you chain a bunch of transformations to create a Directed Acyclic Graph (DAG). The graph is defined lazily, so the actual computation is deferred until an action is called. In this repo, I focus on building the transformation DAG (ie. how do you perform a `groupByKey()` in a lazy way?). The annotations below explain the pros and cons for each of my implementations, which are ordered from simplest to most sophisticated and complete. My notebook requires Python 3.3+. Enjoy!  \n",
    "\n",
    "Disclaimer: This repo does not focus on the parallelized nature of computation across machines.\n",
    "<br><br>\n",
    "\n",
    "\n",
    "#### 1st Attempt\n",
    "Pros: \n",
    "* simple generator style to create transforms\n",
    "  \n",
    "Cons: \n",
    "* only works for first action. Once called, no further actions can be performed\n",
    "\n",
    "#### 2nd Attempt\n",
    "Pros:\n",
    "* 2nd implemention using generator style\n",
    "* slight improvement in that calling an action the second time will return non-empty list. \n",
    "\n",
    "Cons: \n",
    "* still suffers from the same problem of only can perform (correct) action 1 time\n",
    "* only the transformations up to the last transformation are applied--cannot get the correct result from action for first transformation if there exists a second transformation\n",
    "\n",
    "#### 3rd Attempt\n",
    "Pros:\n",
    "* decorator style: composing higher order functions to emulate the sequence of transformations\n",
    "* added `reduce()` action\n",
    "* can create multiple RDDs where actions will get you the correct result\n",
    "* decoupled the RDDs, hence RDDs are immutable\n",
    "\n",
    "Cons:\n",
    "* cannot do `filter()` or `groupByKey()` due to the inherent limitation of using decorators\n",
    "\n",
    "#### 4th Attempt\n",
    "Pros:\n",
    "* instead of generator or decorator, implemented RDD transformations as a list of map functions\n",
    "\n",
    "Cons:\n",
    "* `reduceByKey()`, `filter()`, and `groupByKey()` not implemented\n",
    "\n",
    "#### 5th Attempt\n",
    "Pros:\n",
    "* 2nd implementation using a list of functions and generator function\n",
    "* `reduceByKey()` transformation implemented\n",
    "* `filter()` transformation implemented weakly\n",
    "* attempted to implement `flatMap()`\n",
    "\n",
    "Cons:\n",
    "* `reduceByKey()` uses `groupByKey()`, which creates a dictionary with keys and lists of values. Hence, memory usage can spike if the list of values corresponding to its key is very large\n",
    "* `filter()` doesn't work if transformed element is falsy (False, 0, \"\")\n",
    "* `flatMap()` doesn't work correctly due to limitation of how `__results_generator__()` iterates though elements in RDD\n",
    "\n",
    "#### 6th Attempt (Best Implementation)\n",
    "Pros:\n",
    "* 3rd implementation using a list of functions (and recursive generator function)\n",
    "* superior implementation of `reduceByKey()` where two values for a given key is immediately reduced into 1 value. Hence, each key will always have only 1 value\n",
    "* superior implementation of `filter()` transformation using the recursive generator function `__results_generator__()`\n",
    "* `flatMap()` implementation fixed using the recursive generator function `__results_generator__()`\n",
    "\n",
    "Cons:\n",
    "* can only reify (call an action on) an inputted generator 1 time--if you `parallelize()` a generator, then the second time you call an action, an empty list will be returned. Only a weakness for generators as they can only be iterated through once. If you `parallelize()` a list or tuple, then the second time you call an action, the correct answer will be returned.\n",
    "\n",
    "#### 7th Attempt\n",
    "Pros:\n",
    "* 4th implementation using a  list of functions (and recursive generator function)\n",
    "* even if you `parallelize()` a generator, the second time you call an action, RDD will return the correct result. After the first reify, the generator (that you `parallelize()`) is memoized as a list\n",
    "* Moved the type checking (whether if the current RDD's `__sequence__` is a (generator) function, generator, or container) from `__results_generator__()` to `__sequence_generator__()`\n",
    "\n",
    "Cons:\n",
    "* despite being more complete than the 6th implementation, it is harder to read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_Spark:\n",
    "    def __init__(self):\n",
    "        self.sequence = None\n",
    "        self.rdd = None\n",
    "    \n",
    "    def parallelize(self, sequence):\n",
    "        self.sequence = sequence\n",
    "        return self\n",
    "    \n",
    "    def map(self, func):\n",
    "        if self.rdd is None:\n",
    "            self.rdd = (func(element) for element in self.sequence)\n",
    "        else:\n",
    "            self.rdd = (func(element) for element in self.rdd)\n",
    "        return self\n",
    "    \n",
    "    def collect(self):\n",
    "        results = list(self.rdd)\n",
    "        return results\n",
    "    \n",
    "    def __iter__(self):\n",
    "        yield from self.rdd\n",
    "        \n",
    "    def __contains__(self, element):\n",
    "        return element in iter(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 6, 12, 18, 24, 30, 36, 42, 48, 54]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def multi_2(x): return x * 2\n",
    "def multi_3(x): return x * 3\n",
    "\n",
    "sc = Simple_Spark()\n",
    "rdd = sc.parallelize(range(10))\n",
    "rdd2 = rdd.map(multi_2)\n",
    "rdd3 = rdd2.map(multi_3)\n",
    "\n",
    "print(rdd3.collect())\n",
    "print(rdd3.collect()) # action only works once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "sc = Simple_Spark()\n",
    "rdd = sc.parallelize(range(10))\n",
    "rdd2 = rdd.map(multi_2)\n",
    "rdd3 = rdd2.map(multi_3)\n",
    "print(54 in rdd3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "\n",
    "\n",
    "class Simple_Spark:\n",
    "    def __init__(self):\n",
    "        self.rdd = None\n",
    "    \n",
    "    def parallelize(self, sequence):\n",
    "        self.rdd = sequence\n",
    "        return self\n",
    "    \n",
    "    def map(self, func):\n",
    "        def wrapper(rdd=self.rdd):\n",
    "            if isinstance(rdd, types.FunctionType):\n",
    "                for element in rdd():\n",
    "                    yield func(element)\n",
    "            else:\n",
    "                for element in rdd:\n",
    "                    yield func(element)\n",
    "                    \n",
    "        self.rdd = wrapper\n",
    "        return self\n",
    "        \n",
    "    def collect(self):\n",
    "        if isinstance(self.rdd, types.FunctionType):\n",
    "            results = list(self.rdd())\n",
    "        else:\n",
    "            results = list(self.rdd)\n",
    "        return results\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if isinstance(self.rdd, types.FunctionType):\n",
    "            yield from self.rdd()\n",
    "        else:\n",
    "            yield from self.rdd\n",
    "\n",
    "    def __contains__(self, element):\n",
    "        return element in iter(self)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_2(x): return x * 2\n",
    "def multi_3(x): return x * 3\n",
    "\n",
    "sc = Simple_Spark()\n",
    "rdd = sc.parallelize(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 6, 12, 18, 24, 30, 36, 42, 48, 54]\n",
      "[0, 6, 12, 18, 24, 30, 36, 42, 48, 54]\n"
     ]
    }
   ],
   "source": [
    "rdd2 = rdd.map(multi_2)\n",
    "rdd3 = rdd.map(multi_3)\n",
    "\n",
    "print(rdd2.collect()) # this RDD is now incorect\n",
    "print(rdd3.collect()) # only works once, which is the last map transform apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(10))\n",
    "rdd2 = rdd.map(multi_2)\n",
    "print(8 in rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3rd Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import copy\n",
    "\n",
    "\n",
    "class Simple_Spark:\n",
    "    def parallelize(self, sequence):\n",
    "        return RDD(sequence)\n",
    "\n",
    "\n",
    "class RDD:\n",
    "    def __init__(self, __sequence__, __transformation_chain__=None):\n",
    "        self.__sequence__ = __sequence__\n",
    "        self.__transformation_chain__ = __transformation_chain__\n",
    "\n",
    "    @staticmethod\n",
    "    def __compose__(next_function, previous_function): # decorator\n",
    "        def wrapper(element):\n",
    "            return next_function(previous_function(element))\n",
    "        return wrapper\n",
    "    \n",
    "    def map(self, func): # immutability\n",
    "        if self.__transformation_chain__ is None:\n",
    "            __transformation_chain__ = func\n",
    "        else:\n",
    "            __transformation_chain__ = copy.deepcopy(self.__compose__(func, self.__transformation_chain__))\n",
    "        return RDD(self.__sequence__, __transformation_chain__)\n",
    "    \n",
    "    def reduce(self, func):\n",
    "        return reduce(lambda x, y: func(x, y), self.__results_generator__())\n",
    "\n",
    "    def __results_generator__(self):\n",
    "        if self.__transformation_chain__ is None:\n",
    "            return (element for element in self.__sequence__)\n",
    "        else:\n",
    "            return (self.__transformation_chain__(element) for element in self.__sequence__)\n",
    "\n",
    "    def collect(self):\n",
    "        return list(self.__results_generator__())\n",
    "  \n",
    "    def __contains__(self, element):\n",
    "        return element in self.__results_generator__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n"
     ]
    }
   ],
   "source": [
    "def multi_2(x): return x * 2\n",
    "\n",
    "sc = Simple_Spark()\n",
    "rdd = sc.parallelize(range(10))\n",
    "rdd2 = rdd.map(multi_2)\n",
    "\n",
    "print(rdd.collect())\n",
    "print(rdd2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(rdd2.reduce(lambda x, y: x + y))\n",
    "print(rdd.collect())\n",
    "print(7 in rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]\n"
     ]
    }
   ],
   "source": [
    "print(rdd\n",
    "    .map(lambda x: x * 2)\n",
    "    .map(lambda x: x + 1)\n",
    "    .map(lambda x: x / 2)\n",
    "    .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4th Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import copy\n",
    "\n",
    "\n",
    "class Simple_Spark:\n",
    "    def parallelize(self, sequence):\n",
    "        return RDD(sequence)\n",
    "\n",
    "\n",
    "class RDD:\n",
    "    def __init__(self, __sequence__, __transformation_chain__=None):\n",
    "        self.__sequence__ = __sequence__\n",
    "        self.__transformation_chain__ = [] if __transformation_chain__ is None else __transformation_chain__\n",
    "        \n",
    "    def map(self, func):\n",
    "        return RDD(self.__sequence__, copy.copy(self.__transformation_chain__ + [func]))\n",
    "    \n",
    "    def reduce(self, func):\n",
    "        return reduce(lambda x, y: func(x, y), self.__results_generator__())\n",
    "\n",
    "    def __results_generator__(self):\n",
    "        return (reduce(lambda value, f: f(value), self.__transformation_chain__, element) for element in self.__sequence__)\n",
    "    \n",
    "    def collect(self):\n",
    "        return list(self.__results_generator__())\n",
    "            \n",
    "    def __contains__(self, element):\n",
    "        return element in self.__results_generator__()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_2(x): return x * 2\n",
    "\n",
    "sc = Simple_Spark()\n",
    "rdd = sc.parallelize(range(10))\n",
    "rdd2 = rdd.map(multi_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
      "90\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(rdd.collect())\n",
    "print(rdd2.collect())\n",
    "print(rdd2.reduce(lambda x, y: x + y))\n",
    "print(rdd.collect())\n",
    "print(7 in rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5th Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import types\n",
    "\n",
    "\n",
    "\n",
    "class Simple_Spark:\n",
    "    def parallelize(self, sequence):\n",
    "        return RDD(sequence)\n",
    "\n",
    "\n",
    "class RDD:\n",
    "    class FilteredElement:\n",
    "        pass    \n",
    "    \n",
    "    def __init__(self, __sequence__, __transformation_chain__=None):\n",
    "        self.__sequence__ = __sequence__\n",
    "        self.__transformation_chain__ = [] if __transformation_chain__ is None else __transformation_chain__\n",
    "        \n",
    "    def map(self, func):\n",
    "        return RDD(self.__sequence__, copy.copy(self.__transformation_chain__ + [func]))    \n",
    "    \n",
    "    def flatMap(self, func):\n",
    "        def wrapper(element):\n",
    "            yield from func(element)\n",
    "            #for element in self.__results_generator__():\n",
    "            #    print(\"element {}\".format(element))\n",
    "            #    for sub_element in func(element):\n",
    "            #        yield sub_element\n",
    "        return RDD(self.__sequence__, __transformation_chain__=copy.copy(self.__transformation_chain__ + [func]))\n",
    "    \n",
    "    def filter(self, func):\n",
    "        def wrapper(element):\n",
    "            temp_element = func(element)\n",
    "            if temp_element:\n",
    "                return element\n",
    "            else:\n",
    "                return RDD.FilteredElement()        \n",
    "        return RDD(self.__sequence__, copy.copy(self.__transformation_chain__ + [wrapper]))\n",
    "\n",
    "    def groupByKey(self):\n",
    "        def wrapper():\n",
    "            key_values_dict = defaultdict(list)\n",
    "            for key_value in self.__results_generator__():\n",
    "                if len(key_value) != 2:\n",
    "                    raise Exception(\"Element does not have length 2\")\n",
    "                key_values_dict[key_value[0]].append(key_value[1])\n",
    "            for key_values in key_values_dict.items():\n",
    "                yield key_values\n",
    "        return RDD(wrapper, __transformation_chain__=None)\n",
    "    \n",
    "    def reduceByKey(self, func): # transformation, not action\n",
    "        def wrapper():\n",
    "            for key, values in self.groupByKey().__results_generator__():\n",
    "                yield key, reduce(func, values)\n",
    "        return RDD(wrapper, __transformation_chain__=None)\n",
    "    \n",
    "    def reduce(self, func):\n",
    "        return reduce(lambda x, y: func(x, y), self.__results_generator__())\n",
    "\n",
    "    def __results_generator__(self):\n",
    "        if isinstance(self.__sequence__, types.FunctionType):\n",
    "            __sequence__ = self.__sequence__()\n",
    "        else:\n",
    "            __sequence__ = self.__sequence__\n",
    "        for element in __sequence__:\n",
    "            for func in self.__transformation_chain__:\n",
    "                element = func(element)\n",
    "                if isinstance(element, RDD.FilteredElement):\n",
    "                    break\n",
    "            else:\n",
    "                yield element\n",
    "    \n",
    "    def collect(self):\n",
    "        return list(self.__results_generator__())\n",
    "            \n",
    "    def __contains__(self, element):\n",
    "        return element in self.__results_generator__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n"
     ]
    }
   ],
   "source": [
    "sc = Simple_Spark()\n",
    "\n",
    "rdd = sc.parallelize([range(10), range(10, 20)])\n",
    "\n",
    "print(rdd.map(list).collect())\n",
    "print(rdd.flatMap(list).collect()) # flatMap doesn't work correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 7, 9]\n",
      "[2, 6, 10, 14, 18]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(10))\n",
    "rdd1 = rdd.filter(lambda x: x % 2)\n",
    "print(rdd1.collect())\n",
    "rdd2 = rdd1.map(lambda x: x * 2)\n",
    "print(rdd2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('even', 0), ('odd', 1), ('even', 2), ('odd', 3), ('even', 4), ('odd', 5), ('even', 6), ('odd', 7), ('even', 8), ('odd', 9)]\n",
      "[('even', [0, 2, 4, 6, 8]), ('odd', [1, 3, 5, 7, 9])]\n"
     ]
    }
   ],
   "source": [
    "rdd3 = rdd.map(lambda x: (\"odd\" if x % 2 else \"even\", x))\n",
    "print(rdd3.collect())\n",
    "rdd4 = rdd3.groupByKey()\n",
    "print(rdd4.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('even', 20), ('odd', 25)]\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "rdd5 = rdd3.reduceByKey(lambda x, y: x + y)\n",
    "print(rdd5.collect())\n",
    "print(rdd5.map(lambda x: x[1]).reduce(lambda x, y: x + y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6th Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import types\n",
    "\n",
    "\n",
    "class Simple_Spark:\n",
    "    def parallelize(self, sequence):\n",
    "        return RDD(sequence)\n",
    "\n",
    "\n",
    "class RDD:    \n",
    "    def __init__(self, __sequence__, __transformation_chain__=None):\n",
    "        self.__sequence__ = __sequence__\n",
    "        self.__transformation_chain__ = [] if __transformation_chain__ is None else __transformation_chain__\n",
    "        \n",
    "    def map(self, func):\n",
    "        def wrapper(element):\n",
    "            yield from [func(element)]\n",
    "        return RDD(self.__sequence__, copy.copy(self.__transformation_chain__) + [wrapper])\n",
    "\n",
    "    def flatMap(self, func):\n",
    "        def wrapper(element):\n",
    "            yield from func(element)\n",
    "        return RDD(self.__sequence__, __transformation_chain__=copy.copy(self.__transformation_chain__) + [wrapper])\n",
    "\n",
    "    def filter(self, func):\n",
    "        def wrapper(element):\n",
    "            return filter(func, [element])\n",
    "        return RDD(self.__sequence__, copy.copy(self.__transformation_chain__) + [wrapper])\n",
    "\n",
    "    def groupByKey(self):\n",
    "        def wrapper():\n",
    "            key_values_dict = defaultdict(list)\n",
    "            for key_value in self.__results_generator__():\n",
    "                if len(key_value) != 2:\n",
    "                    raise Exception(\"Element does not have length 2\")\n",
    "                key_values_dict[key_value[0]].append(key_value[1])\n",
    "            yield from key_values_dict.items()\n",
    "        return RDD(wrapper, __transformation_chain__=None)\n",
    "    \n",
    "    def reduceByKey(self, func): # transformation, not action\n",
    "        def wrapper():\n",
    "            reducebykey_dict = {}\n",
    "            for key, value in self.__results_generator__():\n",
    "                if key in reducebykey_dict:\n",
    "                    reducebykey_dict[key] = func(reducebykey_dict[key], value)\n",
    "                else:\n",
    "                    reducebykey_dict[key] = value\n",
    "            yield from reducebykey_dict.items()\n",
    "        return RDD(wrapper, __transformation_chain__=None)\n",
    "\n",
    "    def reduce(self, func):\n",
    "        return reduce(lambda x, y: func(x, y), self.__results_generator__())\n",
    "\n",
    "    def __results_generator__(self):\n",
    "        def __recursive_generator__(element, __transformation_chain__):\n",
    "            if len(__transformation_chain__):\n",
    "                for subelement in __transformation_chain__[0](element):\n",
    "                    yield from __recursive_generator__(subelement, __transformation_chain__[1:])\n",
    "            else:\n",
    "                yield element\n",
    "        \n",
    "        if isinstance(self.__sequence__, types.FunctionType):\n",
    "            __sequence__ = self.__sequence__()\n",
    "        else:\n",
    "            __sequence__ = self.__sequence__\n",
    "        for element in __sequence__:\n",
    "            yield from __recursive_generator__(element, self.__transformation_chain__)\n",
    "\n",
    "    def collect(self):\n",
    "        return list(self.__results_generator__())\n",
    "            \n",
    "    def __contains__(self, element):\n",
    "        return element in self.__results_generator__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n"
     ]
    }
   ],
   "source": [
    "sc = Simple_Spark()\n",
    "\n",
    "rdd = sc.parallelize([range(10), range(10, 20)])\n",
    "\n",
    "print(rdd.map(list).collect())\n",
    "print(rdd.flatMap(list).collect()) # flatMap works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('even', 0), ('odd', 1), ('even', 2), ('odd', 3), ('even', 4), ('odd', 5), ('even', 6), ('odd', 7), ('even', 8), ('odd', 9), ('even', 10), ('odd', 11), ('even', 12), ('odd', 13), ('even', 14), ('odd', 15), ('even', 16), ('odd', 17), ('even', 18), ('odd', 19)]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = rdd.flatMap(list).map(lambda x: (\"odd\" if x % 2 else \"even\", x))\n",
    "print(rdd1.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('even', [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]), ('odd', [1, 3, 5, 7, 9, 11, 13, 15, 17, 19])]\n",
      "True\n",
      "True\n",
      "[('even', 90), ('odd', 100)]\n"
     ]
    }
   ],
   "source": [
    "rdd2 = rdd1.groupByKey()\n",
    "print(rdd2.collect())\n",
    "print(('odd', [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]) in rdd2)\n",
    "print('odd'in rdd2.map(lambda kv: kv[0]))\n",
    "print(rdd1.reduceByKey(lambda x, y: x + y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7th Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import types\n",
    "\n",
    "\n",
    "class Simple_Spark:\n",
    "    def parallelize(self, sequence):\n",
    "        return RDD(sequence)\n",
    "\n",
    "\n",
    "class RDD:    \n",
    "    def __init__(self, __sequence__, __transformation_chain__=None):\n",
    "        self.__sequence__ = __sequence__\n",
    "        self.__transformation_chain__ = [] if __transformation_chain__ is None else __transformation_chain__\n",
    "        \n",
    "    @property\n",
    "    def __sequence_generator__(self):\n",
    "        print(type(self.__sequence__))\n",
    "        if isinstance(self.__sequence__, types.FunctionType):\n",
    "            yield from self.__sequence__()\n",
    "        elif isinstance(self.__sequence__, types.GeneratorType):\n",
    "            print(\"run 1 time\")\n",
    "            placeholder = []\n",
    "            for element in self.__sequence__:\n",
    "                placeholder.append(element)\n",
    "                yield element\n",
    "            self.__sequence__ = placeholder\n",
    "        else:\n",
    "            yield from self.__sequence__\n",
    "\n",
    "    def map(self, func):\n",
    "        def wrapper(element):\n",
    "            yield from [func(element)]\n",
    "        return RDD(lambda: self.__sequence_generator__, copy.copy(self.__transformation_chain__) + [wrapper])\n",
    "    \n",
    "    def flatMap(self, func):\n",
    "        def wrapper(element):\n",
    "            yield from func(element)\n",
    "        return RDD(lambda: self.__sequence_generator__, __transformation_chain__=copy.copy(self.__transformation_chain__) + [wrapper])\n",
    "    \n",
    "    def filter(self, func):\n",
    "        def wrapper(element):\n",
    "            return filter(func, [element])\n",
    "        return RDD(lambda: self.__sequence_generator__, copy.copy(self.__transformation_chain__) + [wrapper])\n",
    "\n",
    "    def groupByKey(self):\n",
    "        def wrapper():\n",
    "            key_values_dict = defaultdict(list)\n",
    "            for key_value in self.__results_generator__():\n",
    "                if len(key_value) != 2:\n",
    "                    raise Exception(\"Element does not have length 2\")\n",
    "                key_values_dict[key_value[0]].append(key_value[1])\n",
    "            yield from key_values_dict.items()\n",
    "        return RDD(wrapper, __transformation_chain__=None)\n",
    "    \n",
    "    def reduceByKey(self, func): # transformation, not action\n",
    "        def wrapper():\n",
    "            reducebykey_dict = {}\n",
    "            for key, value in self.__results_generator__():\n",
    "                if key in reducebykey_dict:\n",
    "                    reducebykey_dict[key] = func(reducebykey_dict[key], value)\n",
    "                else:\n",
    "                    reducebykey_dict[key] = value\n",
    "            yield from reducebykey_dict.items()\n",
    "        return RDD(wrapper, __transformation_chain__=None)\n",
    "    \n",
    "    def reduce(self, func):\n",
    "        return reduce(lambda x, y: func(x, y), self.__results_generator__())\n",
    "\n",
    "    def __results_generator__(self):\n",
    "        def __recursive_generator__(element, __transformation_chain__):\n",
    "            if len(__transformation_chain__):\n",
    "                for subelement in __transformation_chain__[0](element):\n",
    "                    yield from __recursive_generator__(subelement, __transformation_chain__[1:])\n",
    "            else:\n",
    "                yield element\n",
    "\n",
    "        for element in self.__sequence_generator__:\n",
    "            yield from __recursive_generator__(element, self.__transformation_chain__)\n",
    "    \n",
    "    def collect(self):\n",
    "        return list(self.__results_generator__())\n",
    "            \n",
    "    def __contains__(self, element):\n",
    "        return element in self.__results_generator__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "run 1 time\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "sc = Simple_Spark()\n",
    "rdd = sc.parallelize((x for x in range(10)))\n",
    "\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'>\n",
      "<class 'list'>\n",
      "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n"
     ]
    }
   ],
   "source": [
    "rdd2 = rdd.map(lambda x: x * 2)\n",
    "print(rdd2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'list'>\n",
      "[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n"
     ]
    }
   ],
   "source": [
    "print(rdd2.map(lambda x: x / 2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'>\n",
      "<class 'generator'>\n",
      "run 1 time\n",
      "[1, 3, 5, 7, 9]\n",
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'list'>\n",
      "[2, 6, 10, 14, 18]\n"
     ]
    }
   ],
   "source": [
    "gen = (x for x in range(10))\n",
    "\n",
    "rdd = sc.parallelize(gen)\n",
    "rdd2 = rdd.filter(lambda x: x % 2)\n",
    "print(rdd2.collect())\n",
    "\n",
    "rdd3 = rdd2.map(lambda x: x * 2)\n",
    "print(rdd3.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'>\n",
      "<class 'list'>\n",
      "[('even', 0), ('odd', 1), ('even', 2), ('odd', 3), ('even', 4), ('odd', 5), ('even', 6), ('odd', 7), ('even', 8), ('odd', 9)]\n",
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'list'>\n",
      "[('even', [0, 2, 4, 6, 8]), ('odd', [1, 3, 5, 7, 9])]\n"
     ]
    }
   ],
   "source": [
    "rdd4 = rdd.map(lambda x: (\"odd\" if x % 2 else \"even\", x))\n",
    "print(rdd4.collect())\n",
    "\n",
    "rdd5 = rdd4.groupByKey()\n",
    "print(rdd5.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'list'>\n",
      "[('even', 20), ('odd', 25)]\n",
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'list'>\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "rdd6 = rdd4.reduceByKey(lambda x, y: x + y)\n",
    "print(rdd6.collect())\n",
    "\n",
    "print(rdd6.map(lambda x: x[1]).reduce(lambda x, y: x + y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'>\n",
      "<class 'list'>\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'list'>\n",
      "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19]\n",
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'list'>\n",
      "[2, 6, 10, 14, 18, 22, 26, 30, 34, 38]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([range(10), range(10, 20)])\n",
    "\n",
    "rdd2 = rdd.flatMap(list)\n",
    "print(rdd2.collect())\n",
    "rdd3 = rdd2.filter(lambda x: x % 2)\n",
    "print(rdd3.collect())\n",
    "\n",
    "rdd4 = rdd3.map(lambda x: x * 2)\n",
    "print(rdd4.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'list'>\n",
      "[('even', 0), ('odd', 1), ('even', 2), ('odd', 3), ('even', 4), ('odd', 5), ('even', 6), ('odd', 7), ('even', 8), ('odd', 9), ('even', 10), ('odd', 11), ('even', 12), ('odd', 13), ('even', 14), ('odd', 15), ('even', 16), ('odd', 17), ('even', 18), ('odd', 19)]\n",
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'list'>\n",
      "[('even', [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]), ('odd', [1, 3, 5, 7, 9, 11, 13, 15, 17, 19])]\n"
     ]
    }
   ],
   "source": [
    "rdd5 = rdd2.map(lambda x: (\"odd\" if x % 2 else \"even\", x))\n",
    "print(rdd5.collect())\n",
    "\n",
    "rdd6 = rdd5.groupByKey()\n",
    "print(rdd6.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'list'>\n",
      "[('even', 90), ('odd', 100)]\n",
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'list'>\n",
      "190\n",
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'list'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "rdd7 = rdd5.reduceByKey(lambda x, y: x + y)\n",
    "print(rdd7.collect())\n",
    "\n",
    "print(rdd7.map(lambda x: x[1]).reduce(lambda x, y: x + y))\n",
    "\n",
    "print((\"even\", 90) in rdd7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_rdd = (sc.parallelize(x for x in range(10))\n",
    "            .map(lambda x: (\"odd\" if x % 2 else \"even\", x))\n",
    "            .reduceByKey(lambda x, y: x + y)\n",
    "            .map(lambda kv: kv[1])\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'generator'>\n",
      "run 1 time\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_rdd.reduce(lambda x, y: x + y) # generator is reified once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_rdd.reduce(lambda x, y: x + y) # second time, underlying collection is a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_2(x): return x * 2\n",
    "def multi_3(x): return x * 3\n",
    "def multi_4(x): return x * 4\n",
    "\n",
    "rdd = sc.parallelize(range(10))\n",
    "rdd2 = rdd.map(multi_2)\n",
    "rdd3 = rdd2.map(multi_3)\n",
    "rdd4 = rdd3.map(multi_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'>\n",
      "<class 'range'>\n",
      "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
      "<class 'function'>\n",
      "<class 'range'>\n",
      "True\n",
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'range'>\n",
      "[0, 6, 12, 18, 24, 30, 36, 42, 48, 54]\n",
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'range'>\n",
      "270\n",
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'range'>\n",
      "[0.5, 6.5, 12.5, 18.5, 24.5, 30.5, 36.5, 42.5, 48.5, 54.5]\n"
     ]
    }
   ],
   "source": [
    "print(rdd2.collect())\n",
    "print(6 in rdd2)\n",
    "print(rdd3.collect())\n",
    "print(rdd3.reduce(lambda x, y: x + y))\n",
    "print(rdd3\n",
    "    .map(lambda x: x * 2)\n",
    "    .map(lambda x: x + 1)\n",
    "    .map(lambda x: x / 2)\n",
    "    .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'function'>\n",
      "<class 'range'>\n",
      "0 mult2\n",
      "0 mult3\n",
      "0 mult4\n",
      "1 mult2\n",
      "2 mult3\n",
      "6 mult4\n",
      "2 mult2\n",
      "4 mult3\n",
      "12 mult4\n",
      "3 mult2\n",
      "6 mult3\n",
      "18 mult4\n",
      "4 mult2\n",
      "8 mult3\n",
      "24 mult4\n",
      "5 mult2\n",
      "10 mult3\n",
      "30 mult4\n",
      "6 mult2\n",
      "12 mult3\n",
      "36 mult4\n",
      "7 mult2\n",
      "14 mult3\n",
      "42 mult4\n",
      "8 mult2\n",
      "16 mult3\n",
      "48 mult4\n",
      "9 mult2\n",
      "18 mult3\n",
      "54 mult4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 24, 48, 72, 96, 120, 144, 168, 192, 216]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multiply_and_print(multiplicand, comment):\n",
    "    def multiply(number):\n",
    "        print(number, comment)\n",
    "        return multiplicand * number\n",
    "    return multiply\n",
    "\n",
    "rdd = sc.parallelize(range(10))\n",
    "rdd2 = rdd.map(multiply_and_print(2, \"mult2\"))\n",
    "rdd3 = rdd2.map(multiply_and_print(3, \"mult3\"))\n",
    "rdd4 = rdd3.map(multiply_and_print(4, \"mult4\"))\n",
    "\n",
    "rdd4.collect() # look at the order of lazy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
